{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 2 : Pytorch and Recommenders\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with a neural network toolkit.\n",
    "\n",
    "Also, in addition to using only rating, we propose to add text.\n",
    "\n",
    "\n",
    "## WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "```python\n",
    "#initialize an empty 5x3 matrix\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "```\n",
    "\n",
    "```\n",
    "out[]:\n",
    "\n",
    "tensor([[8.3665e+22, 4.5580e-41, 1.6025e-03],\n",
    "        [3.0763e-41, 0.0000e+00, 0.0000e+00],\n",
    "        [0.0000e+00, 0.0000e+00, 3.4438e-41],\n",
    "        [0.0000e+00, 4.8901e-36, 2.8026e-45],\n",
    "        [6.6121e+31, 0.0000e+00, 9.1084e-44]])\n",
    "        \n",
    "```\n",
    "\n",
    "### Most useful functions:\n",
    "\n",
    "\n",
    "```python\n",
    "#initialize an empty 5x3 matrix\n",
    "x = torch.empty(5, 3)\n",
    "print(x.size())\n",
    "```\n",
    "\n",
    "### Full tutorial: \n",
    "\n",
    "a full pytorch tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n",
    "### What's interesting:\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "As usual: first thing to do is to load data: here we use an amazon review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One tuple: user,item,review,rating\n",
      "[('A11N155CW1UV02', 'B000H00VBQ', \"I had big expectations because I love English TV, in particular Investigative and detective stuff but this guy is really boring. It didn't appeal to me at all.\", 2.0)]\n",
      "32485\n",
      "4641\n"
     ]
    }
   ],
   "source": [
    "import gzip , json\n",
    "\n",
    "#Here data is in json format: one \"dict\" per line.\n",
    "def jsons2tuple(s,*keys):\n",
    "    js = json.loads(s)\n",
    "    return tuple([js[k] for k in keys])\n",
    "\n",
    "#we directly read the gzip file\n",
    "with gzip.open(\"dataset/reviews_Amazon_Instant_Video_5.json.gz\",\"r\") as f:\n",
    "    data = [jsons2tuple(x,\"reviewerID\",\"asin\",\"reviewText\",\"overall\") for x in f]\n",
    "\n",
    "#how one exemple looks like\n",
    "print(\"One tuple: user,item,review,rating\")\n",
    "print(data[:1])\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "#we take 80% for train and 20% for valid/test\n",
    "for i,x in enumerate(data):\n",
    "    if i % 8 ==0:\n",
    "        test.append(x)\n",
    "    else:\n",
    "        train.append(x)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prepare Data\n",
    "We loaded raw data, now we prepare it:\n",
    "\n",
    "- (1) user and items are remaped to ids from 0->len(users) /0->len(items)\n",
    "- (2) reviews are tokenized using simple split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "i_dic = {}\n",
    "u_dic = {}\n",
    "word_count = Counter()\n",
    "\n",
    "prep_train = []\n",
    "\n",
    "def text_preprocess(t):\n",
    "    \"\"\"\n",
    "    a function to preprocess the text if needed\n",
    "    takes str, returns list str\n",
    "    \"\"\"\n",
    "    return t.split(\" \")\n",
    "\n",
    "\n",
    "# User and Items to key + split text + Count common words (to prune)\n",
    "\n",
    "for uid,iid,text,rating in train:\n",
    "    uk = u_dic.setdefault(uid,len(u_dic))\n",
    "    ik = i_dic.setdefault(iid,len(i_dic))\n",
    "    ptext = text_preprocess(text)\n",
    "    word_count.update(ptext)    \n",
    "    prep_train.append((uk,ik,ptext,rating))\n",
    "\n",
    "    \n",
    "# Unknown users/items are set to None    \n",
    "    \n",
    "prep_test = []\n",
    "\n",
    "for uid,iid,text,rating in test:\n",
    "    uk = u_dic.get(uid,None)\n",
    "    ik = i_dic.get(iid,None)   \n",
    "    ptext = text_preprocess(text)\n",
    "    prep_test.append((uk,ik,ptext,rating))\n",
    "    \n",
    "\n",
    "# we further divide \"test\" in validation and test set\n",
    "cutout = len(prep_test)//2\n",
    "prep_val = prep_test[:cutout]\n",
    "prep_test = prep_test[cutout:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch Models\n",
    "\n",
    "Now that we have loaded and prepared the data, we can define the models.\n",
    "\n",
    "\n",
    "## 1) Classic SVD (with mean)\n",
    "\n",
    "First we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =   torch.sum(embed_u*embed_i,-1)\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 20\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, _,ratings = zip(*l) #we ignore review text\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,size_average=False)\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(len(u_dic),len(i_dic),num_feat)\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    ## Validation loss (no training)\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval() # Inference mode\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    ## Test loss (no training)\n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = nn.Embedding(nb_users, 1)\n",
    "        self.imean = nn.Embedding(nb_items, 1)\n",
    "        self.gmean =  nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = torch.sum(embed_u*embed_i,1) + umean.squeeze(-1) + imean.squeeze(-1) + self.gmean\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, _ ,ratings = zip(*l) # we ignore reviews for now\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,size_average=False)\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "model =  KorenMF(len(u_dic),len(i_dic),num_feat)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Taking text into account\n",
    "\n",
    "\n",
    "## A) Let's first predict the rating from review text\n",
    "\n",
    "To do so we need to:\n",
    "\n",
    "- (1) Change the collate function to take text into account\n",
    "- (2) Add word embedding in the model\n",
    "\n",
    "\n",
    "#### (1) Complete the new collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the collate function\n",
    "from random import shuffle\n",
    "\n",
    "max_words = 10000\n",
    "word_dic = {k:i for i,(k,v) in enumerate(word_count.most_common(max_words),2)} # word -> id (pad = 0 ,unk=1)\n",
    "\n",
    "def tuple_batch_text(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, reviews,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    \n",
    "    max_len = max([len(rev) for rev in reviews])\n",
    "    max_len = min(max_len,1000)\n",
    "    \n",
    "    reviews_t = torch.LongTensor(len(reviews),max_len).fill_(0)  # what is the dimension of input tensor ?\n",
    "    \n",
    "    for i,rev in enumerate(reviews):\n",
    "        rev_words = [word_dic.get(w,1) for w in rev][:max_len]\n",
    "        rev_t = torch.LongTensor(rev_words)\n",
    "        reviews_t[i,:len(rev_words)] = rev_t\n",
    "    \n",
    "    return users_t,items_t,reviews_t,ratings_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Rating prediction from text model\n",
    "you can use [EmbeddingBag](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) to direclty combine word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class RatingPred(nn.Module):\n",
    "    \n",
    "    def __init__(self,dic_size,word_latent_size):\n",
    "        super(RatingPred, self).__init__()\n",
    "        self.text_emb = nn.EmbeddingBag(dic_size,word_latent_size)\n",
    "        self.to_rating = nn.Linear(word_latent_size,1)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        text_emb = self.text_emb(text)\n",
    "        pred_rating = self.to_rating(text_emb)\n",
    "\n",
    "        return pred_rating.squeeze(-1) ##### Should be a 1-dim tensor of all predicted ratings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RatingPred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90a50dfb51e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRatingPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RatingPred' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "size_embedding = 50\n",
    "lr = 0.001\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t):\n",
    "    '''\n",
    "    mse loss.\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t)\n",
    "    return mse\n",
    "    \n",
    "\n",
    "\n",
    "model = RatingPred(max_words,size_embedding)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_text)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_text)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch_text)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "    length = [len(dataloader_train),len(dataloader_val),len(dataloader_test)]\n",
    "\n",
    "    for _,_,reviews_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "        pred_rating = model(reviews_t)\n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "      \n",
    "        \n",
    "        \n",
    "        mse_loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    for _,_,reviews_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred_rating = model(reviews_t)\n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for _,_,reviews_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred_rating = model(reviews_t)\n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "    \n",
    "    \n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/length[0]).item(),3),\"/\",  round((mean_loss[1]/length[1]).item(),3),\"/\",  round((mean_loss[2]/length[2]).item(),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Let's now predict the rating from review text + Profile\n",
    "\n",
    "To do so we need to:\n",
    "\n",
    "- (1) Add profiles embedding in the model\n",
    "- (2) Change forward function\n",
    "- (3) Add profiles to training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class RatingPredProfile(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_users,nb_items,dic_size,latent_size):\n",
    "        super(RatingPredProfile, self).__init__()\n",
    "        self.text_emb = nn.EmbeddingBag(dic_size,latent_size)\n",
    "        self.to_rating = nn.Linear(latent_size*3,1)\n",
    "        self.users = nn.Embedding(nb_users,latent_size)\n",
    "        self.items = nn.Embedding(nb_items,latent_size)\n",
    "\n",
    "         #init\n",
    "        nn.init.normal_(self.users.weight,0,0.1)\n",
    "        nn.init.normal_(self.items.weight,0,0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, user,item,text):\n",
    "\n",
    "        text_emb = self.text_emb(text)\n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.item(item).squeeze(1)\n",
    "        \n",
    "        concatenation = torch.cat([text_emb,embed_u,embed_i],dim=-1) ######### concatenate them\n",
    "        \n",
    "        pred_rating = selt.to_rating(concatenation)\n",
    "\n",
    "        return pred_rating.squeeze(-1) # 1-dim tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should now call our new model in the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b3d9a308d557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRatingPredProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_dic' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "size_embedding = 50\n",
    "lr = 0.001\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t):\n",
    "    '''\n",
    "    mse loss.\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,size_average=False)\n",
    "    return mse\n",
    "    \n",
    "\n",
    "\n",
    "model = RatingPredProfile(len(u_dic),len(i_dic),max_words,size_embedding)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_text)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_text)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch_text)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for user_t,item_t,reviews_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred_rating = model(user_t,item_t,reviews_t)\n",
    "        \n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "        \n",
    "        \n",
    "        mse_loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    for _,_,reviews_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred_rating = model(user_t,item_t,reviews_t)\n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for _,_,reviews_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred_rating = model(user_t,item_t,reviews_t)\n",
    "        mse_loss = loss_func(pred_rating,ratings_t)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "    \n",
    "    \n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In reality, text can not be used as input as it's written after item consumption:\n",
    "\n",
    "## => Let's predict text instead, using the item embedding:\n",
    "\n",
    "\n",
    "# 3) Predicting Text\n",
    "\n",
    "#### (2) Adding word embeddings to model\n",
    "\n",
    "\n",
    "\n",
    "We propose to use the negative sampling loss : \n",
    "\n",
    "$$ \\big( \\log\\sigma(real) + \\sum\\limits_{i=1}^k\\mathbb{E}_{v_{b}\\sim P_n(\\textbf{w})}\\log\\sigma(-fake) \\big)$$\n",
    "\n",
    "With negative sampling, we \"predict\" text in a sense akin to $k$-NN\n",
    "\n",
    "Simply, we have two cosine distances : $real$ and $fake$\n",
    "\n",
    "- $real$ is the distance between **actual** review words and the predicted embedding \n",
    "- $fake$ is the distance between **fake** sampled word and the predicted embedding\n",
    "\n",
    "\n",
    "The goal is to bring prediction closer to real words than to fake words\n",
    "\n",
    "### Things to do:\n",
    "\n",
    "- (a) Tuple batch should be modified to only consider a fixed number of words\n",
    "- (b) Model should compute the negative sampling loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the collate function\n",
    "from random import shuffle\n",
    "\n",
    "max_words = 11000\n",
    "word_dic = {k:i for i,(k,v) in enumerate(word_count.most_common(max_words)[1000:],2)} # word -> id (pad = 0 ,unk=1)\n",
    "\n",
    "def tuple_batch_piece_text(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, reviews,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    \n",
    "    max_len = 5 ## We only consider a subset of words\n",
    "\n",
    "    reviews_t = ######################\n",
    "    \n",
    "    for i,rev in enumerate(reviews):\n",
    "        rev_words = [word_dic.get(w,1) for w in rev]\n",
    "        shuffle(rev_words)\n",
    "        rev_words = rev_words[:max_len]\n",
    "        rev_t = torch.LongTensor(rev_words)\n",
    "        reviews_t[i,:len(rev_words)] = rev_t\n",
    "    \n",
    "    return users_t,items_t,reviews_t,ratings_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "class TextItem(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_users,nb_items,dic_size,latent_size):\n",
    "        super(TextItem, self).__init__()\n",
    "\n",
    "        self.items =#############\n",
    "        self.text_emb = ########\n",
    "        self.dic_size = ###########\n",
    "       \n",
    "        \n",
    "        #init\n",
    "        nn.init.normal_(self.items.weight,0,0.1)\n",
    "        nn.init.normal_(self.text_emb.weight,0,0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, item, text):\n",
    "        \n",
    "        embed_i = self.items(item).squeeze(1)\n",
    "        \n",
    "        real_text = normalize(self.text_emb(text),2)\n",
    "        fake_text = normalize(self.text_emb(text.clone().random_(2,self.dic_size)),2)\n",
    "\n",
    "        norm_i = normalize(embed_i,2)\n",
    "        \n",
    "        \n",
    "        dot_real = ################# cosine distance btw real and embed_i\n",
    "        dot_fake = ################# cosine distance btw fake and embed_i\n",
    "        \n",
    "        close = ####### left part of Negative Sampling\n",
    "\n",
    "        far = ####### right part of Negative Sampling\n",
    "        \n",
    "\n",
    "        return embed_i, close, far\n",
    "    \n",
    "    def get_text(self,user,item):\n",
    "        \"\"\"\n",
    "        here we embed\n",
    "        \"\"\"\n",
    "        embed_i_text = self.items(item).squeeze(1)                \n",
    "        return embed_i_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "What changes: \n",
    "\n",
    "- (a) Model\n",
    "- (b) Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "size_embedding = 50\n",
    "lr = 0.01\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(close,far):\n",
    "   \n",
    "    error = ######\n",
    "    \n",
    "    return torch.mean(error)\n",
    "    \n",
    "\n",
    "\n",
    "model = ####################\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_piece_text)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch_piece_text)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch_piece_text)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred_txt,close,far = model(items_t,reviews_t)\n",
    "\n",
    "        loss = loss_func(close,far)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred_txt,close,far = model(items_t,reviews_t)\n",
    "        loss = loss_func(close,far)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred_txt,close,far = model(items_t,reviews_t)\n",
    "        loss = loss_func(close,far)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"loss (train/val/test)\", round((mean_loss[0]/len(dataloader_train)).item(),3),\"/\",  round((mean_loss[1]/len(dataloader_val)).item(),3),\"/\",  round((mean_loss[2]/len(dataloader_test)).item(),3))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_word_dic = {i:k for k,i in word_dic.items()}\n",
    "inv_word_dic[0] = \"pad\"\n",
    "inv_word_dic[1] = \"unk\"\n",
    "\n",
    "        \n",
    "def get_most_similar(text_emb,embeddings,dictionnary,top_k=10):\n",
    "    \"\"\"\n",
    "    Returns the k closest embeddings labels \n",
    "    \n",
    "    text_emb  is a tensor (N,)\n",
    "    embeddings is a matrix (M,N)\n",
    "    dictionnary \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    affinity = torch.sum(text_embs * embeddings,-1) \n",
    "    x,ind = torch.sort(affinity)\n",
    "    ind = ind.tolist()\n",
    "    top_k = ind[::-1][:top_k]\n",
    "\n",
    "    return [dictionnary[x] for x in top_k]\n",
    "\n",
    "\n",
    "text_embs = #######\n",
    "embeddings = ########\n",
    "\n",
    "print(get_most_similar(text_embs,embeddings,inv_word_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL Model: Wrapping MF + text prediction: \n",
    "\n",
    "We propose to do the following simple SVD model: \n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "and link it with our previous text prediction method\n",
    "\n",
    "We propose to link via a linear layer on items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassicMF(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_users,nb_items,dic_size,latent_size,word_latent_size):\n",
    "        super(TextClassicMF, self).__init__()\n",
    "        self.users = #########\n",
    "        self.items = ###########\n",
    "        self.text_emb =##############\n",
    "        self.to_text = nn.Linear(latent_size,word_latent_size)\n",
    "        self.mean = ###########\n",
    "        self.dic_size = dic_size\n",
    "        \n",
    "        #init\n",
    "        nn.init.normal_(self.users.weight,0,0.1)\n",
    "        nn.init.normal_(self.items.weight,0,0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, user, item, text):\n",
    "        ####\n",
    "      \n",
    "\n",
    "        return \n",
    "    \n",
    "    def get_text(self,user,item):\n",
    "         \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out = torch.sum(embed_u*embed_i,1) + self.mean\n",
    "        embed_i_text = self.to_text(embed_i)\n",
    "                \n",
    "        return embed_i_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "size_embedding = 50\n",
    "lr = 0.1\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,size_average=False)\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "\n",
    "model = TextClassicMF(len(u_dic),len(i_dic),max_words,num_feat,size_embedding)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=tuple_batch_text)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=tuple_batch_text)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=tuple_batch_text)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t,reviews_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t,reviews_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,reviews_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t,reviews_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "text_embs = #######\n",
    "embeddings = ########\n",
    "\n",
    "print(get_most_similar(text_embs,embeddings,inv_word_dic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
