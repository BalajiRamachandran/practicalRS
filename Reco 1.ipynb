{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 1\n",
    "\n",
    "Here we explore simple matrix factorization models for recommendation \n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "On the internet, people rate items (or simply click on them, showing preference). All these ratings can be seen as a matrix coding for all ratings of size (n_user,n_item).It's worth noting that this matrix is highly sparse: No one rates everything, people only rate a subset of items.\n",
    "\n",
    "Recommendation (more specifically **collaborative filtering**) goal can be seen as predicting how someone will rate one item. This is akin to filling this matrix, by predicting all missing ratings. The most used algorithms for this task are matrix factorization ones: they take advantage from the fact that a matrix can be decomposed into two sub-matrices: one coding for the users and one coding for the items.\n",
    "\n",
    "## Why predicting ratings and not directly items:\n",
    "\n",
    "Theoritically, the goal of a recommender system is to cherry pick interesting items for one user within a huge collection. Rating prediction is a surrogate problem of item prediction. To get what item to recommend you can, for exemple, simply sort best rated items. It has been shown that improving rating prediction actually improve item prediction.\n",
    "\n",
    "\n",
    "\n",
    "## Data: [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "In this practical we use a small dataset of user ratings on movies. Specifically, we treat the dataset as list of $(user,item,rating)$ triplets.\n",
    "\n",
    "\n",
    "## Goals of this practical:\n",
    "\n",
    "- Visualize different learnt latent item space\n",
    "- Implement different matrix factorization algorithms for explicit collaborative filtering\n",
    "- Understand and Implement both prediction and ranking evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Warmup (read & run): Visualize learnt item embeddings with PCA & T-SNE\n",
    "\n",
    "### Load data\n",
    "\n",
    "We need read each line from the CSV to extract $(user,item,rating)$ triplets. Also, each user and item id's has to be remapped to be contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Rating matrices are highly sparse (there are many missing ratings).\n",
    "# Therefore, the use of sparse matrices is recommended.\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Data file is a CSV\n",
    "with open(\"dataset/ml-latest-small/ratings.csv\") as ratings:\n",
    "    rating_data = ratings.readlines()\n",
    "\n",
    "# We separate the header from actual data\n",
    "header = rating_data[0]\n",
    "print(header)\n",
    "\n",
    "# Data is CSV: userID,itemID,rating,timestamp\n",
    "rating_data = rating_data[1:] #contains list(tuple(uid,iid,rating,timestamp))\n",
    "\n",
    "\n",
    "# Helper to transform one \"str\" line to a tuple\n",
    "def line2tuple(l):\n",
    "    uid,iid,rating, timestamp = l.strip().split(\",\")\n",
    "    return (int(uid),int(iid),float(rating), int(timestamp))\n",
    "\n",
    "# generator for one line. (enables direct unpacking)\n",
    "def tl(l):\n",
    "    for x in l:\n",
    "        yield(line2tuple(x))\n",
    "\n",
    "# We first remap users and item to ids between (0,len(user)) and (0,len(item))\n",
    "u_dic = {}\n",
    "i_dic = {}\n",
    "        \n",
    "all_data = []\n",
    "    \n",
    "    \n",
    "for uid,iid,rating,ts in tl(rating_data):  # iterating on all data\n",
    "    uk = u_dic.setdefault(uid,len(u_dic))\n",
    "    ik = i_dic.setdefault(iid,len(i_dic))\n",
    "    all_data.append((uk,ik,float(rating)))\n",
    "\n",
    "num_user = len(u_dic)\n",
    "num_item = len(i_dic)\n",
    "\n",
    "print(\"there are \"+str((num_user,num_item)) +\" users and items\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing item latent space\n",
    "Factorizing a matrices into two submatrices yields embeddings of users and items. These embeddings encode the similarity that can exists between users or between items. They can be visualized with algorithms such as PCA and T-SNE. In this first part, we aim at visualizing obtained item embeddings following a factorization on the entire rating matrix.\n",
    "\n",
    "\n",
    "- To do so, we propose to use the following function `save_embeddings` and [the tensorflow projector](http://projector.tensorflow.org/) to visualize obtained latent features with pca and t-sne.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function saves embeddings (a numpy array) and associated labels into tsv files.\n",
    "\n",
    "def save_embeddings(embs,dict_label,path):\n",
    "    \"\"\"\n",
    "    embs is Numpy.array(N,size)\n",
    "    dict_label is {str(word)->int(idx)} or {int(idx)->str(word)}\n",
    "    \"\"\"\n",
    "    def int_first(k,v):\n",
    "        if type(k) == int:\n",
    "            return (k,v)\n",
    "        else:\n",
    "            return (v,k)\n",
    "\n",
    "    np.savetxt(f\"{path}_vectors.tsv\", embs, delimiter=\"\\t\")\n",
    "\n",
    "    #labels \n",
    "    if dict_label:\n",
    "        sorted_labs = np.array([lab for idx,lab in sorted([int_first(k,v) for k,v in dict_label.items()])])\n",
    "        print(sorted_labs)\n",
    "        with open(f\"{path}_metadata.tsv\",\"w\") as metadata_file:\n",
    "            for x in sorted_labs: #hack for space\n",
    "                if len(x.strip()) == 0:\n",
    "                    x = f\"space-{len(x)}\"\n",
    "                    \n",
    "                metadata_file.write(f\"{x}\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization on full rating matrix: decompose rating matrix with NMF and visualize items\n",
    "Three things to do:\n",
    "    - Create sparse matrix of ratings (from $(user,item,rating)$ tuples)\n",
    "    - Factorize it with NMF into U (user sub-matrix) and I (item sub-matrix)\n",
    "    - Extract item labels (movie titles) and save embeddings/labels with preceding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "num_user = len(u_dic)\n",
    "num_item = len(i_dic)\n",
    "\n",
    "# (1) Create sparse matrix from all ratings\n",
    "Full = dok_matrix((num_user, num_item), dtype=np.float32)\n",
    "\n",
    "for uid,iid,rating in all_data:\n",
    "    Full[uid,iid] = float(rating)\n",
    "    \n",
    "    \n",
    "# (2) Factorizing matrix\n",
    "\n",
    "model = NMF(n_components=25, init='random', random_state=0, max_iter=350)\n",
    "U = model.fit_transform(Full) #users\n",
    "I = model.components_      #items\n",
    "\n",
    "I = I.transpose()\n",
    "I.shape\n",
    "\n",
    "# (3) Loading labels and saving embeddings + vectors\n",
    "\n",
    "\n",
    "# data is csv with header\n",
    "with open(\"dataset/ml-latest-small/movies.csv\") as movies:\n",
    "    movie_data = movies.readlines()\n",
    "\n",
    "# print header\n",
    "print(movie_data[0])\n",
    "\n",
    "# id-> title dictionnary\n",
    "movie_names = {}\n",
    "\n",
    "for x in movie_data[1:]:\n",
    "    iid, title, *rest =  x.split(',')\n",
    "    \n",
    "    iid = float(iid)\n",
    "    \n",
    "\n",
    "    if iid in i_dic:\n",
    "        movie_names[i_dic[iid]] = title\n",
    "\n",
    "\n",
    "####################################\n",
    " \n",
    "# Saving into \"item_50_vectors.tsv\" and \"item_50_metadata.tsv\".\n",
    "save_embeddings(I,movie_names,\"item_50\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize on TensorFlow Projector:\n",
    "[Saved vectors/label can be visualized in tensorflow projector](http://projector.tensorflow.org/)\n",
    "\n",
    "> Files are saved by default as \"item_50_vectors.tsv\" and \"item_50_metadata.tsv\" in root jupyter directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "\n",
    "# (Explicit) Collaborative Filtering  by predicting ratings:\n",
    "\n",
    "The Netflix competition introduced this predictive framework: the goal is to predict missing ratings using existing ones.\n",
    "\n",
    "##  Vanilla NMF AND SVD for rating prediction\n",
    "\n",
    "First, we test two simple models:\n",
    "- Non Negative Matrix Factorization\n",
    "- SVD\n",
    "\n",
    "Given a real rating $r_{ui}$ and a predicted rating $\\hat{r_{ui}}$ on an **observed** couple $(u,i)$ \n",
    "\n",
    "Rating prediction's goal is to minimize mean the prediction error over each $N$ **observed** ratings\n",
    "## $$\\min\\limits_{U,I} \\frac{1}{N}\\sum\\limits_{(u,i)} (r_{ui} -  \\hat{r_{ui}})^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Building Train/Test set **\n",
    "\n",
    "To train/test our models, we first split our data into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We take 10% of the train set as test data\n",
    "train_mat = dok_matrix((num_user, num_item), dtype=np.float32)\n",
    "test = []\n",
    "train = []\n",
    "    \n",
    "for i,(uid,iid,rating) in enumerate(all_data):\n",
    "    if i%10 == 0: #one out of 10 is for test\n",
    "        test.append((uid,iid,rating))\n",
    "    else:\n",
    "        train.append((uid,iid,rating))\n",
    "        train_mat[uid,iid] = rating\n",
    "    \n",
    "print(\"Number of train examples: \", train_mat.nnz)\n",
    "print(\"Number of test examples: \", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Evaluating error **:\n",
    "\n",
    "The goal is to minimize the mean difference between the $n$ predicted and true ratings (respectively $\\hat{r_{ui}}, r_{ui}$). \n",
    "\n",
    "Two common metrics are the Mean Squared Error (MSE) $$\\frac{1}{n}\\sum^1_n (r_{ui}-\\hat{r_{ui}})^2$$ and the Mean Absolute Error (MAE) $$\\frac{1}{n}\\sum^1_n abs(r_{ui}-\\hat{r_{ui}})$$.\n",
    "\n",
    "## 1 ) Complete error functions (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take as input two lists of ratings\n",
    "\n",
    "def MSE_err(truth,pred):\n",
    "    \"\"\"\n",
    "    computes MSE from real-pred difference\n",
    "    \"\"\"\n",
    "    return ########\n",
    "\n",
    "def MAE_err(truth,pred):\n",
    "    \"\"\"\n",
    "    computes MAE from real-pred difference\n",
    "    \"\"\"\n",
    "    return ############\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, we propose have the simple following **prediction rule**: the predicted rating is simply the dot product of the considered user and item embeddings\n",
    "\n",
    "## $$\\hat{r_{ui}} = <U_u^\\top.I_i> $$\n",
    "\n",
    "This is akin to decomposing the training matrix into two sub-matrices using SVD.\n",
    "\n",
    "## Pointers on what to do: \n",
    "\n",
    "1. complete prediction function\n",
    "- fit nmf model and get User and Item profiles to make a prediction\n",
    "- fit svd model and get User and Item profiles to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "\n",
    "print(\"----------------------NMF---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "## NMF model\n",
    "model = NMF(n_components=100, solver='cd' ,random_state=0, max_iter=100,alpha=5,l1_ratio=0.5)\n",
    "\n",
    "#get submatrices\n",
    "U_nmf = \n",
    "I_nmf = \n",
    "\n",
    "print(U_nmf.shape) ## see the shapes of your submatrices\n",
    "print(I_nmf.shape)\n",
    "\n",
    "\n",
    "## to complete\n",
    "def pred_func_nmf(uid,iid):\n",
    "    \n",
    "    Uu = #\n",
    "    Ii = #\n",
    "    \n",
    "    return ##  \n",
    "\n",
    "\n",
    "## Getting the truth values\n",
    "truth_tr = np.array([rating for (uid,iid),rating in train_mat.items()])\n",
    "truth_te = np.array([rating for uid,iid,rating in test])\n",
    "\n",
    "prediction_tr = np.array([####### for (u,i),rating in train_mat.items()])\n",
    "prediction_te = np.array([####### for u,i,rating in test])\n",
    "\n",
    "\n",
    "print(\"Training Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "    \n",
    "print(\"Test Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "print(\"MAE:\",  MAE_err(prediction_te,truth_te))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"----------------------SVD---------------------------\")\n",
    "\n",
    "## SVD Model\n",
    "\n",
    "model = TruncatedSVD(n_components=150)\n",
    "\n",
    "#get submatrices\n",
    "U_svd = ####\n",
    "I_svd = ####\n",
    "\n",
    "\n",
    "def pred_func_svd(uid,iid):\n",
    "    \n",
    "    Uu = #\n",
    "    Ii = #\n",
    "    \n",
    "    return #  \n",
    "\n",
    "    \n",
    "prediction_tr = ######\n",
    "prediction_te = ####\n",
    "\n",
    "\n",
    "print(\"Training Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "    \n",
    "print(\"Test Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "print(\"MAE:\",  MAE_err(prediction_te,truth_te))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and visualize nmf & svd embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_embeddings(###,movie_names,\"svd\")\n",
    "save_embeddings(###,movie_names,\"nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "# Normalization and Regularization\n",
    "\n",
    "\n",
    "Trying to decompose a sparse matrix this way is highly prone to overfitting: The train error is low, but the test error is much higher. Traditionnally, models are highly regularized to achieve better performances.\n",
    "\n",
    "$$\n",
    "\\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  I_i^\\top U_u)^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2)}_\\text{regularization}\n",
    "$$\n",
    "\n",
    "Here we were trying to factorize rating patterns within latent profiles. However, it is common to mean normalize each rating to model \"mean deviation\" instead. In the next part of this practical session, we will explore two mean normalization techniques for rating prediction. \n",
    "\n",
    "**Mean normalization** is a common way to improve performance in rating prediction. In fact, the global average is one of the simplest baseline to try when doing rating prediction\n",
    "\n",
    "\n",
    "## (TODO) Mean baseline:\n",
    "\n",
    "\n",
    "To get the mean baseline:\n",
    "\n",
    "- (1) compute training set mean\n",
    "- (2) complete prediction function\n",
    "\n",
    "The prediction rule simply is:\n",
    "## $$\\hat{r_{ui}} = \\mu  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"----------------------MEAN ONLY---------------------------\")\n",
    "\n",
    "\n",
    "# compute mean training ratings (~3.5)\n",
    "mean = ###\n",
    "\n",
    "\n",
    "def pred_func_mean(uid,iid):\n",
    "    \n",
    "    \n",
    "    return ###\n",
    "\n",
    "print(\"mean rating is \", mean)\n",
    "\n",
    "\n",
    "prediction_tr = ####\n",
    "prediction_te = ####\n",
    "\n",
    "print(\"Training Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "    \n",
    "print(\"Test Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "print(\"MAE:\",  MAE_err(prediction_te,truth_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, using only the mean ratings to predict future ratings has much MUCH better performances than our two previous models...\n",
    "\n",
    "### (TODO) Taking the mean rating into account:\n",
    "\n",
    "Most work on rating prediction model mean ratings. The goal is to only factor deviation from the mean instead of modeling the full rating pattern.\n",
    "\n",
    "To take into account the mean, the easiest way is to substract it by doing the following:\n",
    "\n",
    "- (1) compute training set mean\n",
    "- (2) remove mean from training rating matrix\n",
    "- (3) factorize the normalized matrix\n",
    "- (4) when predicting a rating, simply add the mean\n",
    "\n",
    "The prediction rule becomes:\n",
    "## $$\\hat{r_{ui}} = \\mu  + <U_u^\\top.I_i> $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "\n",
    "# (1) compute mean of training ratings\n",
    "mean = ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (2) mean normalize training matrix\n",
    "tmn = dok_matrix((num_user, num_item), dtype=np.float32)\n",
    "\n",
    "for (uid,iid), rating in train_mat.items():\n",
    "    tmn[uid,iid] = ######\n",
    "\n",
    "# (3) factorize matrix\n",
    "model_norm = TruncatedSVD(n_components=150)\n",
    "\n",
    "#get submatrices\n",
    "U_msvd =######\n",
    "I_msvd = ####\n",
    "\n",
    "def pred_func_msvd(uid,iid): \n",
    "    \n",
    "    Uu = ####\n",
    "    Ii = ####\n",
    "    \n",
    "    return ######\n",
    "\n",
    "\n",
    "prediction_tr = ####\n",
    "prediction_te = ####\n",
    "\n",
    "\n",
    "print(\"Training Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "    \n",
    "print(\"Test Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "print(\"MAE:\",  MAE_err(prediction_te,truth_te))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Seminal) Koren 2009 model:\n",
    "\n",
    "the following model propose to even more refine the bias. In addition to the mean bias, it adds a bias for each user and for each item. The goal is to only factorize the mean rating deviation. This model also enables a more finely grained baseline estimate:\n",
    "\n",
    "\n",
    "> Suppose that you want\n",
    "a first-order estimate for user Joe’s rating of the movie\n",
    "Titanic. Now, say that the average rating over all movies, µ,\n",
    "is 3.7 stars. Furthermore, Titanic is better than an average\n",
    "movie, so it tends to be rated 0.5 stars above the average.\n",
    "On the other hand, Joe is a critical user, who tends to rate\n",
    "0.3 stars lower than the average. Thus, the estimate for\n",
    "Titanic’s rating by Joe would be 3.9 stars (3.7 + 0.5 - 0.3)\n",
    "\n",
    "The prediction rule is the following:\n",
    "\n",
    "### $$r_{ui} = \\mu + b_i + b_u + U_u^\\top .I_i $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mu$ is the general mean\n",
    "- $b_i$ is the item's mean deviation from the general mean : $\\mu-\\mu_i$\n",
    "- $b_u$ is the user's mean deviation from the general mean : $\\mu-\\mu_u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "\n",
    "def group_by_user(tuple_list):\n",
    "    r_dic = {}\n",
    "    for uid,iid,rating in tuple_list:\n",
    "        list_rev = r_dic.get(uid,[])\n",
    "        list_rev.append(rating)\n",
    "    \n",
    "        r_dic[uid] =list_rev\n",
    "    return r_dic\n",
    "\n",
    "\n",
    "def group_by_item(tuple_list):\n",
    "    r_dic = {}\n",
    "    for uid,iid,rating in tuple_list:\n",
    "        list_rev = r_dic.get(iid,[])\n",
    "        list_rev.append(rating)\n",
    "    \n",
    "        r_dic[iid] =list_rev\n",
    "    return r_dic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (1) compute means of training set\n",
    "mean = ###\n",
    "\n",
    "# user and item deviation to mean\n",
    "u_means = ####\n",
    "i_means = ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (2) normalize training matrix\n",
    "tmn_k = dok_matrix((num_user, num_item), dtype=np.float32)\n",
    "\n",
    "for (uid,iid), rating in train_mat.items():\n",
    "    tmn_k[uid,iid] = #####\n",
    "\n",
    "# (3) factorize matrix\n",
    "model_kor = TruncatedSVD(n_components=150)\n",
    "\n",
    "\n",
    "U_ksvd = ####\n",
    "I_ksvd = ####\n",
    "\n",
    "\n",
    "def pred_func_ksvd(uid,iid):\n",
    "    Uu = ####\n",
    "    Ii = ####\n",
    "    \n",
    "    return ##########\n",
    "\n",
    "\n",
    "prediction_tr = ###\n",
    "prediction_te = ###\n",
    "\n",
    "\n",
    "print(\"Training Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "    \n",
    "print(\"Test Error:\")\n",
    "print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "print(\"MAE:\",  MAE_err(prediction_te,truth_te))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and visualize nmf & svd embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_embeddings(###,movie_names,\"svd\")\n",
    "save_embeddings(###,movie_names,\"nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, by far, the best model (test mse $\\approx$ 0.804) .\n",
    "\n",
    "\n",
    "### Takeways:\n",
    "\n",
    "In general, when predicting ratings, it is critical to model rating bias. Its also worth noting that other biases exists. Notably, people don't rate items at random. Taking into account **what** people rate is at least as important as taking into account **how** people rate. See [SVD++](http://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf), a model which models, in addition to rating biais, implicit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------\n",
    "\n",
    "# Collaborative Filtering and Ranking Metrics\n",
    "\n",
    "Most of the time recommender systems present $k$ items to the users. Therefore, recommenders are often evaluated by how many relevant items are in their $k$ set (using ranking metrics).\n",
    "\n",
    "Different ranking methods exists such as the [Mean Reciprocal Rank](http://en.wikipedia.org/wiki/Mean_reciprocal_rank) or the [normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)\n",
    "\n",
    "Here we focus on the latter as it can take into account rating scores.\n",
    "\n",
    "## (TODO) Implementing nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The dcg@k is the sum of the relevance, penalized gradually\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        \n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        \n",
    "    return 0.\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# dcg_at_k(r, 1) => 3.0\n",
    "# dcg_at_k(r, 2) => 4.2618595071429155\n",
    "    \n",
    "\n",
    "# And it's normalized version\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "    \"\"\"\n",
    "    dcg_max =  # TO COMPLETE \n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# ndcg_at_k(r, 1) => 1.0\n",
    "# ndcg_at_k(r, 4) => 0.794285\n",
    "    \n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]    \n",
    "ndcg_at_k(r, 4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) compute nDCG on test lists.\n",
    "\n",
    "One first thing to do is to compute the nDCG on our tests list. This enables us to answer the following question:\n",
    "\n",
    "- Can our model properly sort what the user has seen ? (But hasn't been train on)\n",
    "\n",
    "To compare ourselves, we first build a random baseline which simply shuffles test ratings.\n",
    "\n",
    "\n",
    "=> What is nDCG for every model we tried ? ?\n",
    "\n",
    "\n",
    "### Pointers:\n",
    "\n",
    "Here, we first compute average ndcg on **test set only**. The goal is simply to see how well our model sorts what has been seen. Here are the steps to follow:\n",
    "\n",
    "1. We group each test set $(user,item,rating)$ by user -> this gives you a list of items per user\n",
    "2. Then, using previous models prediction functions, you predict each items' rating\n",
    "3. Using those provided ratings, you can sort the items\n",
    "4. Using these sorted items, you can have the sorted list of real ratings to compute ndcg.\n",
    "\n",
    "**tip:** `np.argsort(array)[::-1]]` does inverse argsort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "#1) Group (uid,iid,rating) per uid\n",
    "def group_by_user(tuple_list):\n",
    "    r_dic = {}\n",
    "    for uid,iid,rating in tuple_list:\n",
    "        list_rev = r_dic.get(uid,[])\n",
    "        list_rev.append((uid,iid,rating))\n",
    "    \n",
    "        r_dic[uid] =list_rev\n",
    "    return r_dic #returns {uid:[(uid,iid,rating),...],...}\n",
    "\n",
    "\n",
    "\n",
    "userg_train = group_by_user(train)  #returns {uid:[(uid,iid,rating),...],...}\n",
    "userg_test = group_by_user(test)\n",
    "\n",
    "\n",
    "# Function to compute a random shuffle ndcg\n",
    "def random_ndcg(uid_group_tuples,k=10):\n",
    "    mean_ndcg = 0\n",
    "    num_users = 0\n",
    "    \n",
    "    #for each test set\n",
    "    for _,list_rating in uid_group_tuples.items():\n",
    "\n",
    "        #shuffle real ratings.\n",
    "        real_ratings = [rating for uid,iid,rating in list_rating]\n",
    "        shuffle(real_ratings)\n",
    "        pred_objects = real_ratings\n",
    "\n",
    "        mean_ndcg += ndcg_at_k(pred_objects,k)\n",
    "        num_users += 1\n",
    "\n",
    "    return  mean_ndcg/num_users\n",
    "\n",
    "\n",
    "\n",
    "#Function to compute ndcg on test set\n",
    "def mean_ndcg_UI(U,I,pred_function,uid_group_tuples,k=10):\n",
    "    mean_ndcg = 0\n",
    "    num_users = 0\n",
    "    \n",
    "    #for each test set\n",
    "    for _,list_rating in uid_group_tuples.items():\n",
    "        \n",
    "        #2)compute predictions\n",
    "        pred_ratings = []\n",
    "        \n",
    "        #3)to sort real ratings\n",
    "        real_ratings = [rating for uid,iid,rating in list_rating]\n",
    "        pred_objects = [] ### complete here, we want the predicted rating lists\n",
    "        \n",
    "        #4)and compute ndcg\n",
    "        mean_ndcg += ndcg_at_k(pred_objects,k)\n",
    "        num_users += 1\n",
    "\n",
    "    return  mean_ndcg/num_users\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"train\") \n",
    "print(\"-\"*10)\n",
    "print(\"mean == random\", random_ndcg(userg_train))\n",
    "print(\"ndcg nmf\", mean_ndcg_UI(U_nmf,I_nmf,pred_func_nmf,userg_train)) \n",
    "print(\"ndcg svd\", mean_ndcg_UI(U_svd,I_svd,pred_func_svd,userg_train))\n",
    "print(\"ndcg svd + mean\", mean_ndcg_UI(U_msvd,I_msvd,pred_func_nmf,userg_train))\n",
    "print(\"ndcg svd koren\", mean_ndcg_UI(U_ksvd,I_ksvd,pred_func_ksvd,userg_train))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"test\")    \n",
    "print(\"-\"*10) \n",
    "print(\"mean == random\", random_ndcg(userg_test))\n",
    "print(\"ndcg nmf\", mean_ndcg_UI(U_nmf,I_nmf,pred_func_nmf,userg_test)) \n",
    "print(\"ndcg svd\", mean_ndcg_UI(U_svd,I_svd,pred_func_svd,userg_test))\n",
    "print(\"ndcg svd + mean\", mean_ndcg_UI(U_msvd,I_msvd,pred_func_nmf,userg_test))\n",
    "print(\"ndcg svd koren\", mean_ndcg_UI(U_ksvd,I_ksvd,pred_func_ksvd,userg_test))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The koren model should outperform every other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nDCG on full item set\n",
    "When computing the nDCG on the test list, you assume that you already know what the user will rate. In a real setting, this information is unknown. All you know is what the user already rated and it's up to you to find both  **what** and **how** people will rate.\n",
    "\n",
    "One obvious baseline here is the **item popularity**, where we simply rank items by their number of ratings. Unseen items in test set are considered not interesting and have a score of 0\n",
    "\n",
    "Let's see how our model perform in terms of nDCG on the full (unseen) item set:\n",
    "\n",
    "### Pointers:\n",
    "\n",
    "Here, we compute average ndcg on **unseen item set**. The goal is simply to see how well our model sorts what has not been seen. Here are the steps to follow:\n",
    "\n",
    "1. Using previous models prediction functions, you predict each unseen items' rating\n",
    "3. Using those provided ratings, you can sort the items\n",
    "4. Using these sorted items, you can have the sorted list of real ratings to compute ndcg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(iid for _,iid,_ in train)\n",
    "\n",
    "#the popularity predictor\n",
    "def pop_pred(uid,iid):\n",
    "    return counts[iid]\n",
    "\n",
    "#Random ndcg, return a shuffled lists of all possible ratings\n",
    "def random_ndcg_full(k=10,default=0):\n",
    "    mean_ndcg = 0\n",
    "    num_users = 0\n",
    "    \n",
    "    for _,list_rating in userg_test.items():\n",
    "\n",
    "        #all possible ratings\n",
    "        real_ratings = [rating for uid,iid,rating in list_rating] + [default]*(num_item - len(list_rating) - len(userg_train[uid]))\n",
    "        shuffle(real_ratings)\n",
    "        pred_objects = real_ratings\n",
    "        \n",
    "        mean_ndcg += ndcg_at_k(pred_objects,k)\n",
    "        num_users += 1\n",
    "\n",
    "    return  mean_ndcg/len(userg_test)\n",
    "\n",
    "\n",
    "def mean_ndcg_UI_FULL(U,I,pred_function,k=10,default=0):\n",
    "    mean_ndcg = 0\n",
    "    \n",
    "    test_users = set(uid for uid,_,_ in test)\n",
    "    \n",
    "    for user in test_users:\n",
    "        u_train_set = set(iid for _,iid,_ in userg_train[uid])\n",
    "        u_test_dic =  {iid:rating for uid,iid,rating in userg_test[uid]}\n",
    "        \n",
    "        pred_ratings = []\n",
    "        real_ratings = []\n",
    "        \n",
    "        \n",
    "        for item in range(###):\n",
    "            \n",
    "            if item in u_train_set:\n",
    "                continue\n",
    "            else:\n",
    "                p_rating = ######\n",
    "            \n",
    "            pred_ratings.append(p_rating)\n",
    "            real_ratings.append(u_test_dic.get(item,default))\n",
    "            \n",
    "        pred_objects = [] ##### complete here\n",
    "        \n",
    "        \n",
    "        mean_ndcg += ndcg_at_k(pred_objects,k)\n",
    "\n",
    "    return  mean_ndcg/len(test_users)\n",
    "\n",
    "print(\"mean == random\", random_ndcg_full())\n",
    "print(\"ndcg pop\", mean_ndcg_UI_FULL(U_nmf,I_nmf,pop_pred)) \n",
    "print(\"ndcg nmf\", mean_ndcg_UI_FULL(U_nmf,I_nmf,pred_func_nmf)) \n",
    "print(\"ndcg svd\", mean_ndcg_UI_FULL(U_svd,I_svd,pred_func_svd))\n",
    "print(\"ndcg svd + mean\", mean_ndcg_UI_FULL(U_msvd,I_msvd,pred_func_nmf))\n",
    "print(\"ndcg svd koren\", mean_ndcg_UI_FULL(U_msvd,I_msvd,pred_func_ksvd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the random baseline is way out. However, more surprisingly, our best model for rating prediction has the worst score when computing such nDCG, while it had the best one one unseen data. \n",
    "\n",
    "## Takeways:\n",
    "\n",
    "When doing recommendation you have two tasks: \n",
    "\n",
    "- predicting how someone will like something (this is the rating prediction)\n",
    "- predicting what someone will see\n",
    "\n",
    "Matrix factorization algorithms trained on rating prediction **without** taking into account what has been rated do not perform well on predicting what will be seen. Different algorithms exists to solve the task of predicting **what** will be seen. The most famous one is [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/abs/1205.2618). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) w2v as a recommender:\n",
    "\n",
    "Word2vec can be used to encode item co-occurences. Then, using the cosine similarity you can predict which items are similar to seen training items. This is a simple yet effective way to provide item recommendations\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "Find hyperparameters that works better\n",
    "\n",
    "### 1) First, we train a word2vec model where the text is replaced by items id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#words are in fact item ids\n",
    "text = [ [str(iid) for _,iid,rating in l if rating >= 3] for u,l in userg_train.items()]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                size=100, window=2,               ### here we train a cbow model \n",
    "                                min_count=3,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=15,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=0,\n",
    "                                iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The similarity can then be used to find similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_similar_ids(iid,num=5):\n",
    "    if str(iid) in w2v.wv.vocab:\n",
    "        return [int(iid) for iid,_ in w2v.most_similar(str(iid),topn=num)] \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "get_similar_ids(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def mean_ndcg_w2v_FULL(n=2,k=10,default=0):\n",
    "    mean_ndcg = 0\n",
    "    \n",
    "    test_users = set(uid for uid,_,_ in test)\n",
    "    \n",
    "    for user in test_users:\n",
    "        u_train_set = set(iid for _,iid,rating in userg_train[uid] if rating >= 4)\n",
    "        u_test_dic =  {iid:rating for uid,iid,rating in userg_test[uid]}\n",
    "        \n",
    "        pred_items = set(iid for ids in u_train_set for iid in get_similar_ids(ids,n) if iid not in u_train_set)\n",
    "        not_pred = set(iid for x in range(num_item) if (x not in u_train_set and x not in pred_items))\n",
    "        pred_items = list(pred_items) \n",
    "\n",
    "        pred_objects = [ u_test_dic.get(rid,default) for rid in list(pred_items) + list(not_pred)]\n",
    "        \n",
    "        \n",
    "        mean_ndcg += ndcg_at_k(pred_objects,k)\n",
    "\n",
    "    return  mean_ndcg/len(test_users)\n",
    "\n",
    "print(\"ndcg w2v simple\", mean_ndcg_w2v_FULL())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways \n",
    "This works ok, but still not better than popularity baseline. Our dataset is small and our method to get items from similarity is far too simple.\n",
    "\n",
    "\n",
    "### Finally:\n",
    "Try and visualize learnt embeddings (is there something to see ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###save_embeddings(,movie_names,\"w2v\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
